{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in v3 data which is a dictionary of aa and what do they stand for\n",
    "v3 = pd.read_csv('~/Documents/persona_project_cardiology/original data and supplements/v3_segments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v3['name']=v3['CATEGORY2']+\" \"+v3['CATEGORY3'].astype('str')+' '+v3['SEGMENT_NAME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in persona data\n",
    "persona =pd.read_csv('~/Documents/persona_project_cardiology/original data and supplements/persona_exploration.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in persona data with each aa as a feature column\n",
    "persona_aa=pd.read_csv('~/Documents/persona_project_cardiology/clustering/AllUniqueAAToDevice.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#goal is to add segment name into persona aa table and concat them into one long string to do NLP\n",
    "\n",
    "#extract aa id from persona\n",
    "aa=persona_aa.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove 'ID_' from segment id\n",
    "aa1=aa.['SEGMENT_ID'].str.split('_')[1].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put aa with id and aa without id into one dataframe \n",
    "aa2=pd.concat([aa, aa1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge aa with segment id name\n",
    "aa3=aa2.merge(v3, left_on='SEGMENT_ID', right_on='SEGMENT_ID', how='left')\n",
    "#make sure the order of aa3 is the same as the columns of persona aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in column 0, assign segment name in dictionary to aa if the cell equals to 1 \n",
    "persona_aa[persona_aa.iloc[:,0]==1]=aa3.name.iloc[0]\n",
    "\n",
    "#if it is 0 then change it no empty space\n",
    "persona_aa[persona_aa.iloc[:,0]==0]=' ' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put above condition assignments into loop (loop through columns)\n",
    "for i in range(persona_aa.shape[1]):\n",
    "    persona_aa[persona_aa.iloc[:,i]==1]=aa3.name.iloc[0]\n",
    "    persona_aa[persona_aa.iloc[:,i]==0]=' ' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put all columns into one long string \n",
    "df = persona_aa.astype(str).values.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aa_name=' '.join(map(str, aa_df1['name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "vect = text.TfidfVectorizer(analyzer='word',       \n",
    "                             min_df=10,                        # minimum reqd occurences of a word \n",
    "                             stop_words='english',             # remove stop words\n",
    "                             lowercase=True,                   # convert all words to lowercase\n",
    "                             token_pattern='[a-zA-Z0-9]{3,}',  # num chars > 3\n",
    "                             max_features=50000,             # max number of uniq words\n",
    "                            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = vect.fit_transform(aa_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components=10, learning_method=\"batch\")\n",
    "X_lda = lda.fit_transform(dat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
